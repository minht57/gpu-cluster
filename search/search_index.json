{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GPU Cluster","text":"<p>This website introduces a setup of the GPU Cluster, which a powerful web-based system designed for interactive deep learning development. With this platform, users can easily create, test, and deploy advanced machine learning and deep learning models, all through a simple web browser interface. </p> <p>This guide offers a walkthrough of the cluster installation process, following the Zero to JupyterHub with Kubernetes guideline. Additionally, we provide a practical guideline tailored to our specific use case. Explore the world of interactive deep learning development with ease and precision on our GPU Cluster platform.</p> <p>Contributor: Duong Nguyen, Tri Phan, and Timothy Liu.</p>"},{"location":"arch/architecture/","title":"Architecture","text":""},{"location":"arch/architecture/#physical-connection","title":"Physical connection","text":"<ul> <li> <p>IP adresses:</p> <ul> <li> <p>Headnode: cluster-hn</p> <ul> <li>br1: 192.168.33.5</li> <li>br2: 172.16.0.5</li> </ul> </li> <li> <p>Compute node: cluster-gn[1/2/3/..]</p> <ul> <li>br0: 172.20.0.[<code>10 + Node ID</code>]</li> <li>br1: 192.168.33.[<code>10 + Node ID</code>]</li> <li>br2: 172.16.0.[<code>10 + Node ID</code>]</li> </ul> </li> <li> <p>Virtual node: cluster-vgn[1/2/3/..]</p> <ul> <li>br0: 172.20.0.[<code>100 + i</code>]</li> <li>br1: 192.168.33.[<code>100 + i</code>]</li> <li>br2: 172.16.0.[<code>100 + i</code>]</li> </ul> </li> </ul> </li> </ul>"},{"location":"commands/commands/","title":"Usefull commands","text":""},{"location":"commands/commands/#kvm","title":"KVM","text":"<ul> <li>List all VMs</li> </ul> <pre><code>virsh list --all\n</code></pre> <ul> <li>Start VM</li> </ul> <pre><code>virsh start cluster-vgu1\n</code></pre> <ul> <li>Shutdown VM</li> </ul> <pre><code>virsh shutdown cluster-vgu1\n</code></pre> <ul> <li>Force to shutdown</li> </ul> <pre><code>virsh destroy cluster-vgu1\n</code></pre> <ul> <li>List all IP address</li> </ul> <pre><code>virsh net-dhcp-leases br0\n</code></pre> <ul> <li>Access serial terminal</li> <li>To exit: Ctrl + ]</li> </ul> <pre><code>virsh console cluster-vgu1\n</code></pre> <ul> <li>Remove VM and delete its storage</li> </ul> <pre><code>virsh undefine cluster-vgu1 --remove-all-storage\n</code></pre> <ul> <li>Change VM name</li> </ul> <pre><code>virsh domrename cluster-vgu1 cluster-vgu2\n</code></pre> <ul> <li>Check state</li> </ul> <pre><code>virsh domstate cluster-vgu1\n</code></pre> <ul> <li>Export config</li> </ul> <pre><code>virsh dumpxml cluster-vgu5 &gt; aimc-template.xml\n</code></pre> <ul> <li>Check virtual disk</li> </ul> <pre><code>virsh domblklist cluster-vgu5\n</code></pre> <ul> <li>Define new VM</li> </ul> <pre><code>virsh define aimc-template.xml\n</code></pre> <p>NOTE: If it reports the error that the CPU is not compatible with host CPU, then the CPU of the VM needs to be modified. Replace CPU configuration with <code>&lt;cpu mode='host-passthrough' check='none'/&gt;</code> <code>bash   virsh edit guest_name</code></p> <ul> <li>Clone VM:</li> </ul> <pre><code>virt-clone --original cluster-vgu1 --name cluster-vgu1  --file /mnt/local/kvm/images/cluster-vgu2.qcow2\n</code></pre> <ul> <li>vGPU template</li> </ul> <pre><code>    &lt;hostdev mode='subsystem' type='mdev' managed='no' model='vfio-pci' display='off'&gt;\n      &lt;source&gt;\n        &lt;address uuid='26216876-d01a-4eed-b06d-341abcf4da00'/&gt;\n      &lt;/source&gt;\n    &lt;/hostdev&gt;\n</code></pre>"},{"location":"commands/commands/#change-hostname","title":"Change hostname","text":"<pre><code>sudo nano /etc/hostname\n\nsudo nano /etc/hosts\n\nsudo hostname cluster-vgu5\n\nsudo nano /etc/netplan/00-installer-config.yaml\n\nreboot\n</code></pre>"},{"location":"commands/commands/#to-prevent-a-node-from-scheduling-new-pods-use","title":"To prevent a node from scheduling new pods use:","text":"<pre><code>kubectl cordon &lt;node-name&gt;\n\nkubectl uncordon &lt;node-name&gt;\n</code></pre>"},{"location":"commands/commands/#resize-vm-disk","title":"Resize VM disk","text":""},{"location":"commands/commands/#on-host","title":"On host","text":"<pre><code>ls -al /mnt/nfs-en2/kvm/images/cluster-vgu1.qcow2 \nsudo qemu-img info  /mnt/nfs-en2/kvm/images/cluster-vgu1.qcow2\nsudo qemu-img resize  /mnt/nfs-en2/kvm/images/cluster-vgu1.qcow2 +50G\nsudo qemu-img info  /mnt/nfs-en2/kvm/images/cluster-vgu1.qcow2\nsudo fdisk -l /mnt/nfs-en2/kvm/images/cluster-vgu1.qcow2\n</code></pre>"},{"location":"commands/commands/#on-vm","title":"On VM","text":"<pre><code>lsblk\nsudo pvs\nsudo apt install cloud-guest-utils\nsudo growpart /dev/vda 3\nsudo lsblk \nsudo pvresize /dev/vda3\nsudo vgs\nsudo lvextend -r -l +100%FREE  /dev/mapper/ubuntu--vg-ubuntu--lv\nsudo resize2fs  /dev/mapper/ubuntu--vg-ubuntu--lv\ndf -hT | grep mapper\n</code></pre>"},{"location":"commands/commands/#generate-ssh-key","title":"Generate ssh key","text":"<pre><code>ssh-keygen -t rsa\n</code></pre>"},{"location":"commands/commands/#nfs","title":"NFS","text":"<ul> <li>if you cannot mount the nfs, try to add <code>ip route</code></li> </ul> <pre><code>ip route add 172.0.0.0/8 via 172.20.0.1 dev enp1s0\nshowmount -e  172.16.0.5\nmount 172.16.0.5:/mnt/nfs-en2/ /mnt/nfs\nls /mnt/nfs/\n</code></pre>"},{"location":"commands/commands/#kube-debug","title":"Kube debug","text":"<pre><code># Nodes\nkubectl get nodes --all-namespaces -o wide\n\n# Pods\nkubectl get pods -n hub -o wide\n\n# Services\nkubectl get services --all-namespaces\n\n# Describe\nkubectl describe pod &lt;pod_name&gt; -n hub\n\n# Log\nkubectl logs &lt;pod_name&gt; -n hub\n\n# Create a new token\nkubeadm token create --print-join-command\n\n# Restart a namespaces\nkubectl -n kube-system rollout restart deploy\n</code></pre>"},{"location":"commands/commands/#check-port","title":"Check port","text":"<pre><code>sudo lsof -i tcp\nsudo netstat -ntlp\nnc -z -v 172.16.0.5 8081\n</code></pre>"},{"location":"commands/commands/#fail-to-load-nvidia-griddservice-due-to-time-issues","title":"Fail to load <code>nvidia-gridd.service</code> due to time issues","text":"<ul> <li>If the pod is failed to pull new images, there may be have an issues of full disk --&gt; Resize VM disk</li> <li>If <code>nvidia-gridd.service</code> have an issue related to clock, please set a realtime clock</li> </ul> <pre><code>timedatectl set-ntp off\ntimedatectl set-time '2023-06-20 16:14:50'\n</code></pre> <ul> <li>If all vGPU are attached and <code>nvidia-smi -q</code> shows \"Unlicensed\", please check the permission of the <code>.tok</code> file (correct permission is <code>744</code>).</li> </ul>"},{"location":"commands/commands/#helm","title":"Helm","text":"<pre><code>helm show values jupyterhub/jupyterhub &gt; values.yaml\n</code></pre>"},{"location":"new_images/new_images/","title":"Creating new images","text":"<p>Please check out this repo ai-container  to create new images.</p> <p>Details will be updated soon.</p>"},{"location":"setup/jupyter/","title":"JyputerHub installation","text":""},{"location":"setup/jupyter/#kubernetes-storage","title":"Kubernetes storage:","text":"<ul> <li>Reference: https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner#with-helm</li> </ul> <pre><code>sudo mkdir /mnt/nfs\n\nhelm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/\n\nhelm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\\n    --set nfs.server=172.16.0.5 \\\n    --set nfs.path=/mnt/nfs-ehn1/\n\nkubectl patch storageclass nfs-client -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n</code></pre> <ul> <li>Check:</li> </ul> <pre><code>kubectl get storageclass\n</code></pre> <ul> <li>Change the ReclaimPolicy</li> </ul> <pre><code>kubectl get storageclass nfs-client -o yaml &gt; storage-config.yaml\n</code></pre> <ul> <li>Edit a <code>reclaimPolicy</code> field to <code>Retain</code>, and update the config</li> </ul> <pre><code>kubectl replace -f storage-config.yaml --force\n</code></pre>"},{"location":"setup/jupyter/#create-a-shared-storage","title":"Create a shared storage","text":"<ul> <li>Create a config file</li> </ul> <pre><code>nano pvc-shared-folder.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: jupyterhub-shared-volume\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Gi\n</code></pre> <ul> <li>Create PVC for the shared folder</li> </ul> <pre><code>kubectl apply -f pvc-shared-folder.yaml --namespace=hub\n</code></pre>"},{"location":"setup/jupyter/#install-jupyterhub-helm-chart","title":"Install JupyterHub Helm chart","text":"<ul> <li>Config file:</li> <li><code>client_secret</code>: <code>client_token</code> of OAuth Plugin in Moodle</li> <li>Change the IP and port appropriately</li> <li>In <code>Authenticator/admin_users</code>, list of accounts which have the admin permission.</li> </ul> <pre><code>hub:\n  revisionHistoryLimit:\n  config:\n    GenericOAuthenticator:\n      client_id: JupyterHub\n      client_secret: 92745f3e63c073b4170700e773a5ccdadd821f2f811ea597\n      oauth_callback_url: http://192.168.33.5/hub/oauth_callback\n      authorize_url: http://192.168.33.5:8888/moodle/local/oauth/login.php?client_id=jupyterhub&amp;response_type=code\n      token_url: http://192.168.33.5:8888/moodle/local/oauth/token.php\n      userdata_url: http://192.168.33.5:8888/moodle/local/oauth/user_info.php\n      #userdata_method: \"GET\"\n      scope:\n        - user_info\n    Authenticator:\n      admin_users:\n        - admin\n    JupyterHub:\n      authenticator_class: generic-oauth\n  networkPolicy:\n    enabled: false\n\n# singleuser relates to the configuration of KubeSpawner which runs in the hub\n# pod, and its spawning of user pods such as jupyter-myusername.\nsingleuser:\n  allowPrivilegeEscalation: false\n  storage:\n    type: dynamic\n    static:\n      pvcName:\n      subPath: \"{username}\"\n    capacity: 10Gi\n    homeMountPath: /home/jovyan\n    dynamic:\n      storageClass:\n      pvcNameTemplate: claim-{username}{servername}\n      volumeNameTemplate: volume-{username}{servername}\n      storageAccessModes: [ReadWriteOnce]\n    extraVolumes:\n      - name: jupyterhub-shared\n        persistentVolumeClaim:\n          claimName: jupyterhub-shared-volume\n    extraVolumeMounts:\n      - name: jupyterhub-shared\n        mountPath: /home/jovyan/shared\n\n  image:\n    name: tlkh/ai-container\n    tag: \"05.23\"\n  startTimeout: 3600\n  cpu:\n    limit: 4\n    guarantee: 1\n  memory:\n    limit: 4G\n    guarantee: 1G\n  extraResource:\n    limits: {\"nvidia.com/gpu\": \"1\"}\n    guarantees: {\"nvidia.com/gpu\": \"1\"}\n  cmd: jupyterhub-singleuser\n  profileList: []\n  networkPolicy:\n    enabled: false\n\nscheduling:\n  userScheduler:\n    enabled: false\n\ncull:\n  enabled: true\n  users: true # --cull-users\n  adminUsers: true # --cull-admin-users\n  timeout: 3600 # --timeout\n  every: 600 # --cull-every\n  concurrency: 10 # --concurrency\n  maxAge: 0 # --max-age\n</code></pre> <ul> <li>Install JupyterHub using Helm chart</li> </ul> <pre><code>helm repo add jupyterhub https://hub.jupyter.org/helm-chart/\nhelm repo update\nhelm upgrade --cleanup-on-fail   --install hub jupyterhub/jupyterhub   --namespace hub   --create-namespace   --values config.yaml --version 2.0\n</code></pre> <ul> <li>The output should be</li> </ul> <pre><code>### Post-installation checklist\n\n  - Verify that created Pods enter a Running state:\n\n      kubectl --namespace=hub get pod\n\n    If a pod is stuck with a Pending or ContainerCreating status, diagnose with:\n\n      kubectl --namespace=hub describe pod &lt;name of pod&gt;\n\n    If a pod keeps restarting, diagnose with:\n\n      kubectl --namespace=hub logs --previous &lt;name of pod&gt;\n\n  - Verify an external IP is provided for the k8s Service proxy-public.\n\n      kubectl --namespace=hub get service proxy-public\n\n    If the external ip remains &lt;pending&gt;, diagnose with:\n\n      kubectl --namespace=hub describe service proxy-public\n\n  - Verify web based access:\n\n    You have not configured a k8s Ingress resource so you need to access the k8s\n    Service proxy-public directly.\n\n    If your computer is outside the k8s cluster, you can port-forward traffic to\n    the k8s Service proxy-public with kubectl to access it from your\n    computer.\n\n      kubectl --namespace=hub port-forward service/proxy-public 8080:http\n\n    Try insecure HTTP access: http://localhost:8080\n</code></pre> <ul> <li>Config <code>apache2</code></li> <li> <p>Create a new file     <code>bash     sudo vi /etc/apache2/sites-available/jupyterhub.conf</code></p> </li> <li> <p>Check port of JupyterHub     <code>bash     kubectl get services -n hub</code></p> </li> <li> <p>The output should be     <code>bash     aimc@aimc-en1:~$ kubectl get services -n hub     NAME           TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE     hub            ClusterIP      10.110.122.85    &lt;none&gt;        8081/TCP       3d4h     proxy-api      ClusterIP      10.103.58.117    &lt;none&gt;        8001/TCP       3d4h     proxy-public   LoadBalancer   10.109.161.235   &lt;pending&gt;     80:30669/TCP   3d4h</code></p> </li> <li> <p>Copy content and change the appropriate port (port displayed at the <code>proxy-public</code> service)     ```bash              ProxyPreserveHost On <pre><code>RewriteEngine on\nRewriteCond %{HTTP:UPGRADE} ^WebSocket$ [NC]\nRewriteCond %{HTTP:CONNECTION} ^Upgrade$ [NC]\nRewriteRule .* ws://192.168.33.5:30669%{REQUEST_URI} [P]\n\nProxyPass / http://192.168.33.5:30669/\nProxyPassReverse / http://192.168.33.5:30669/\nProxyRequests on\nRequestHeader set X-Forwarded-Proto \"http\"\n\nServerName localhost\n</code></pre> <p> <code>- Enable site</code>bash sudo a2ensite jupyterhub.conf ```</p> <li> <p>Restart <code>apache2</code> <code>bash     sudo systemctl restart apache2</code></p> </li>"},{"location":"setup/kubenetes/","title":"Kubernetes installation","text":"<p>This setup is applied for both a master node and worker node</p>"},{"location":"setup/kubenetes/#installation","title":"Installation","text":"<ul> <li>Install SSH</li> </ul> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\nsudo apt install openssh-server ca-certificates curl gnupg lsb-release apt-transport-https -y\n\nsudo service ssh start\n</code></pre> <ul> <li>Disable swap and comment out the swap partition in <code>/etc/fstab</code></li> </ul> <pre><code>sudo swapoff -a\n</code></pre> <ul> <li>Docker installation</li> </ul> <pre><code>sudo apt-get remove docker docker-engine docker.io containerd runc\nsudo mkdir -p /etc/apt/keyrings\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\nsudo apt-get update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\nsudo usermod -aG docker $USER\n</code></pre> <ul> <li> <p>Container runtime for Kubernetes</p> <ul> <li>Reference this repo.<pre><code>cd\ngit clone https://github.com/Mirantis/cri-dockerd.git\nwget https://storage.googleapis.com/golang/getgo/installer_linux\nchmod +x ./installer_linux\n./installer_linux\nsource ~/.bash_profile\ncd cri-dockerd\nmkdir bin\ngo build -o bin/cri-dockerd\nmkdir -p /usr/local/bin\n\nsudo install -o root -g root -m 0755 bin/cri-dockerd /usr/local/bin/cri-dockerd\nsudo cp -a packaging/systemd/* /etc/systemd/system\n\nsudo sed -i -e 's,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,' /etc/systemd/system/cri-docker.service\n\nsudo systemctl daemon-reload\nsudo systemctl enable cri-docker.service\nsudo systemctl enable --now cri-docker.socket\n</code></pre> </li> </ul> </li> <li> <p>Deploy GPU plugin</p> <ul> <li>Reference this repo</li> <li> <p>Install the <code>nvidia-container-toolkit</code></p> <pre><code>distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/libnvidia-container/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sudo tee /etc/apt/sources.list.d/libnvidia-container.list\nsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit\n</code></pre> </li> <li> <p>Configure <code>docker</code>: <code>sudo vi /etc/docker/daemon.json</code></p> <pre><code>```json\n{\n    \"default-runtime\": \"nvidia\",\n    \"runtimes\": {\n        \"nvidia\": {\n            \"path\": \"/usr/bin/nvidia-container-runtime\",\n            \"runtimeArgs\": []\n        }\n    }\n}\n```\n</code></pre> </li> <li> <p>Restart <code>docker</code>:</p> <pre><code>sudo systemctl restart docker\n</code></pre> </li> <li> <p>Configure <code>containerd</code>: <code>sudo vi /etc/containerd/config.toml</code></p> <ul> <li>Comment out <code>disabled_plugins = [\"cri\"]</code><pre><code>version = 2\n[plugins]\n  [plugins.\"io.containerd.grpc.v1.cri\"]\n    [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n      default_runtime_name = \"nvidia\"\n\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes]\n        [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia]\n          privileged_without_host_devices = false\n          runtime_engine = \"\"\n          runtime_root = \"\"\n          runtime_type = \"io.containerd.runc.v2\"\n          [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia.options]\n            BinaryName = \"/usr/bin/nvidia-container-runtime\"\n</code></pre> </li> </ul> </li> <li> <p>Restart <code>containerd</code>:</p> <pre><code>sudo systemctl restart containerd\n</code></pre> </li> <li> <p>On MASTER only</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.0/nvidia-device-plugin.yml\n</code></pre> </li> </ul> </li> <li> <p>Kubernetes:</p> <ul> <li>Reference this link<pre><code># ONE LINE\ncurl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-archive-keyring.gpg\n\n# ONE LINE\necho \"deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\n\nsudo apt-get update --allow-unauthenticated\nsudo apt-get install -y kubelet kubeadm kubectl --allow-unauthenticated\nsudo apt-mark hold kubelet kubeadm kubectl\n</code></pre> </li> </ul> </li> <li> <p>NFS</p> </li> </ul> <pre><code>sudo apt install nfs-common\n</code></pre> <ul> <li>Reboot</li> </ul>"},{"location":"setup/kubenetes/#kube-master-setup","title":"kube master setup","text":"<p>These steps is set up in the master node ONLY.</p> <ul> <li><code>kube</code> init</li> </ul> <pre><code>sudo kubeadm init --cri-socket=unix:///var/run/cri-dockerd.sock --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=172.16.0.16\n</code></pre> <ul> <li>To check config:</li> </ul> <pre><code>sudo cat /etc/kubernetes/kubelet.conf\n</code></pre> <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <ul> <li>Flannel network plugin:</li> </ul> <pre><code>kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n</code></pre> <ul> <li>Allow pods to schedule on master:</li> </ul> <pre><code>kubectl taint nodes --all node-role.kubernetes.io/control-plane-\n</code></pre>"},{"location":"setup/kubenetes/#helm-installation","title":"Helm installation","text":"<ul> <li>Reference this link.</li> </ul> <pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre>"},{"location":"setup/kubenetes/#setup-new-nodes","title":"Setup new nodes","text":"<ul> <li> <p>Prerequisite:</p> <ul> <li>Need internet for setup. Just be on Kube network should be ok</li> <li>Install Ubuntu 22.04 + NVIDIA GPU driver (included with Ubuntu)</li> <li>Install Kubernetes</li> </ul> </li> <li> <p>Run command on Master</p> </li> </ul> <pre><code>kubeadm token create --print-join-command\n</code></pre> <ul> <li>Reset Kube on new nodes:</li> </ul> <pre><code>sudo kubeadm reset --cri-socket=unix:///var/run/cri-dockerd.sock\n</code></pre> <ul> <li>Clear old configurations</li> </ul> <pre><code>sudo -i\nkubeadm reset\niptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X\nip link set cni0 down\nip link delete cni0 type bridge\nsystemctl stop kubelet\nsystemctl stop docker\niptables --flush\niptables -tnat --flush\nsystemctl start kubelet\nsystemctl start docker\nexit\n</code></pre> <ul> <li>Run the generated command on new nodes:<ul> <li>NOTE: Note the need to add <code>--cri-socket=unix:///var/run/cri-dockerd.sock</code> to the command</li> </ul> </li> </ul> <pre><code>sudo XXXX --cri-socket=unix:///var/run/cri-dockerd.sock\n</code></pre> <ul> <li>Checking<ul> <li>Run <code>kubectl get nodes -o wide</code> to get the status of all nodes</li> <li>GPU functionality check &amp; count:<pre><code>kubectl get nodes -o=custom-columns=NAME:.metadata.name,GPUs:.status.capacity.'nvidia\\.com/gpu'\n</code></pre> </li> </ul> </li> </ul>"},{"location":"setup/kvm/","title":"KVM (Kernel-based Virtual Machine)","text":""},{"location":"setup/kvm/#information","title":"Information","text":"<ul> <li>Ubuntu server 22.04</li> <li>Name convension: <ul> <li>VM node: cluster-vgn[1/2/3/..]<ul> <li>vgn: virtual GPU node</li> </ul> </li> </ul> </li> <li>IP addresses:<ul> <li>Virtual Node:<ul> <li>br0: 172.20.0.[100 + i]</li> <li>br1: 192.168.33.[100 + i]</li> <li>br2: 172.16.0.[100 + i]</li> </ul> </li> </ul> </li> </ul>"},{"location":"setup/kvm/#host-machine","title":"Host machine","text":""},{"location":"setup/kvm/#install-kvm","title":"Install kvm","text":"<ul> <li>Update</li> </ul> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\nsudo apt install bridge-utils qemu-kvm virtinst libvirt-daemon virt-manager -y\n</code></pre> <ul> <li>Check KVM</li> </ul> <pre><code>admin@cluster-gn1:~$ kvm-ok\nINFO: /dev/kvm exists\nKVM acceleration can be used\n</code></pre> <ul> <li>Check <code>libvirtd.service</code></li> </ul> <pre><code>admin@cluster-gn1:~$ sudo systemctl status libvirtd.service \n[sudo] password for admin: \n\u25cf libvirtd.service - Virtualization daemon\n     Loaded: loaded (/lib/systemd/system/libvirtd.service; enabled; vendor preset: enabled)\n     Active: active (running) since Tue 2023-06-13 03:40:55 UTC; 1 day 23h ago\nTriggeredBy: \u25cf libvirtd-admin.socket\n             \u25cf libvirtd-ro.socket\n             \u25cf libvirtd.socket\n       Docs: man:libvirtd(8)\n             https://libvirt.org\n   Main PID: 2806 (libvirtd)\n      Tasks: 23 (limit: 32768)\n     Memory: 56.8M\n        CPU: 13.964s\n     CGroup: /system.slice/libvirtd.service\n             \u251c\u25002806 /usr/sbin/libvirtd\n             \u251c\u25003157 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/br0.conf --leasefile-ro --dhc&gt;\n             \u2514\u25003158 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/br0.conf --leasefile-ro --dhc&gt;\n\nJun 13 03:40:56 cluster-gn1 dnsmasq[3157]: reading /etc/resolv.conf\nJun 13 03:40:56 cluster-gn1 dnsmasq[3157]: using nameserver 127.0.0.53#53\nJun 13 03:40:56 cluster-gn1 dnsmasq[3157]: read /etc/hosts - 7 addresses\nJun 13 03:40:56 cluster-gn1 dnsmasq[3157]: read /var/lib/libvirt/dnsmasq/br0.addnhosts - 0 addresses\nJun 13 03:40:56 cluster-gn1 dnsmasq-dhcp[3157]: read /var/lib/libvirt/dnsmasq/br0.hostsfile\nJun 13 03:40:56 cluster-gn1 libvirtd[2806]: libvirt version: 8.0.0, package: 1ubuntu7.5 (Marc Deslauriers &lt;&gt;\nJun 13 03:40:56 cluster-gn1 libvirtd[2806]: hostname: cluster-gn1\nJun 13 03:40:56 cluster-gn1 libvirtd[2806]: Tried to update an unsupported keyword YA: skipping.\nJun 13 03:40:56 cluster-gn1 libvirtd[2806]: Tried to update an unsupported keyword YA: skipping.\n</code></pre>"},{"location":"setup/kvm/#config-network","title":"Config network","text":"<ul> <li>Create a <code>br0.xml</code> file</li> </ul> <pre><code>&lt;network&gt;\n  &lt;name&gt;br0&lt;/name&gt;\n  &lt;uuid&gt;427c954b-4993-4554-bb1d-79d99882a9a6&lt;/uuid&gt;\n  &lt;forward mode='nat'&gt;\n    &lt;nat&gt;\n      &lt;port start='1024' end='65535'/&gt;\n    &lt;/nat&gt;\n  &lt;/forward&gt;\n  &lt;bridge name='br0' stp='on' delay='0'/&gt;\n  &lt;mac address='52:54:00:0d:ff:76'/&gt;\n  &lt;ip address='172.20.0.1' netmask='255.255.255.0'&gt;\n    &lt;dhcp&gt;\n      &lt;range start='172.20.0.2' end='172.20.0.254'/&gt;\n    &lt;/dhcp&gt;\n  &lt;/ip&gt;\n&lt;/network&gt;\n</code></pre> <ul> <li>Create network</li> </ul> <pre><code>virsh net-define br0.xml\nvirsh net-start br0\nvirsh net-autostart br0\n</code></pre> <ul> <li>Check network</li> </ul> <pre><code>aimc-hpc-gn4@aimc-hpc-gn4:~/kvm$ virsh net-list\n Name   State    Autostart   Persistent\n-----------------------------------------\n br0    active   yes         yes\n</code></pre> <ul> <li>These next commands will delete the default private network, this is not required but you can if you prefer to delete it.</li> </ul> <pre><code>virsh net-destroy default\nvirsh net-undefine default\n</code></pre> <ul> <li>Restart  libvirt daemon</li> </ul> <pre><code>sudo systemctl restart libvirtd.service\n</code></pre>"},{"location":"setup/kvm/#create-vms","title":"Create VMs","text":"<ul> <li>Create an <code>images</code> folder</li> </ul> <pre><code>sudo mkdir -p /mnt/local/kvm/images\n</code></pre> <ul> <li>Script details</li> </ul> <pre><code>sudo virt-install --name vgn1 \\\n--os-variant ubuntu22.04 \\\n--vcpus 18 \\\n--memory 215040 \\\n--location /mnt/local/kvm/ubuntu-22.04.2-live-server-amd64.iso,kernel=casper/vmlinuz,initrd=casper/initrd \\\n--network bridge=br0,model=virtio \\\n--network bridge=br1,model=virtio \\\n--network bridge=br2,model=virtio \\\n--disk /mnt/local/kvm/images/vgn1.qcow2,size=150 \\\n--graphics vnc \\\n--extra-args='console=ttyS0,115200n8 --- console=ttyS0,115200n8' \\\n--debug\n</code></pre> <ul> <li>Please change the appropriate name in the script (arguments: <code>--name</code> and <code>--disk</code>)</li> <li>This will waiting for finishing installtion from GUI</li> </ul>"},{"location":"setup/kvm/#open-vnc-to-setup","title":"Open VNC to setup","text":"<ul> <li>Create an SSH tunnel</li> </ul> <pre><code>ssh admin-gn1@192.168.33.11 -NfL 5900:127.0.0.1:5900\n</code></pre> <ul> <li>Download VNC from https://www.realvnc.com/en/connect/download/viewer/</li> <li>Install</li> <li>Open VM using this URL: 127.0.0.1:5900</li> <li>Install as normal installation</li> </ul>"},{"location":"setup/kvm/#finish-the-creation-of-a-new-vm","title":"Finish the creation of a new VM","text":"<ul> <li>After choosing <code>Reboot</code>, open another terminal and SSH to the native machine (<code>aimc-en1</code> in this case)</li> <li>Go in the console: <code>virsh console ven1</code></li> <li>Press <code>Enter</code> to reboot the VM</li> <li>Press <code>Ctrl</code> + <code>]</code> to exit</li> </ul>"},{"location":"setup/kvm/#new-vms","title":"New VMs","text":""},{"location":"setup/kvm/#set-up-network","title":"Set up network","text":"<ul> <li>Create and edit the config file</li> </ul> <pre><code>vi /etc/netplan/00-installer-config.yaml\n</code></pre> <ul> <li>Choose the IP address from <code>x.x.x.[100 + VM ID]</code></li> </ul> <pre><code>network:\n  ethernets:\n    enp1s0:\n      addresses:\n      - 172.20.0.101/24\n      nameservers:\n        addresses: []\n        search: []\n      routes:\n      - to: default\n        via: 172.20.0.1\n    enp2s0:\n      addresses:\n      - 172.16.0.101/24\n      nameservers:\n        addresses: []\n        search: []\n      routes:\n      - to: default\n        via: 172.16.0.1\n    enp20s0:\n        addresses:\n        - 192.168.33.101/24\n        nameservers:\n          addresses: []\n        routes:\n          - to: default\n            via: 192.168.33.1\n  version: 2\n</code></pre> <ul> <li>Apply change</li> </ul> <pre><code>sudo netplan generate\nsudo netplan apply\n</code></pre>"},{"location":"setup/kvm/#references","title":"References","text":"<ul> <li>https://www.wpdiaries.com/kvm-on-ubuntu/</li> <li>https://www.wpdiaries.com/ubuntu-on-kvm/</li> <li>https://deploy.equinix.com/developers/guides/kvm-and-libvirt</li> </ul>"},{"location":"setup/moodle/","title":"Moodle installation","text":""},{"location":"setup/moodle/#install-slam-stack","title":"Install SLAM stack","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo apt install apache2\nsudo apt install mysql-server mysql-client\nsudo apt install mariadb-server php-mysql\n#sudo apt install php libapache2-mod-php php-mysql\n#sudo apt install php-curl php-json php-cgi\n\nsudo apt install graphviz aspell ghostscript clamav php8.1-fpm php8.1-cli php8.1-pspell php8.1-curl php8.1-gd php8.1-intl php8.1-mysql php8.1-xml php8.1-xmlrpc php8.1-ldap php8.1-zip php8.1-soap php8.1-mbstring\n</code></pre>"},{"location":"setup/moodle/#moodle","title":"Moodle","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo add-apt-repository ppa:ondrej/php\nsudo apt-get update\n\nsudo apt install php8.1 libapache2-mod-php8.1\nsudo a2enmod php8.1\n\ncd /opt/\nsudo git clone https://github.com/moodle/moodle.git\n\nsudo git branch --track MOODLE_402_STABLE origin/MOODLE_402_STABLE\nsudo git checkout MOODLE_402_STABLE\nsudo cp -R /opt/moodle /var/www/html/\nsudo chmod -R 0777 /var/www/html/moodle\nsudo mkdir /var/moodledata\nsudo chown -R www-data /var/moodledata\nsudo chmod -R 0777 /var/moodledata\n</code></pre>"},{"location":"setup/moodle/#setup-database-for-moodle","title":"Setup database for moodle","text":"<pre><code>sudo mysql -u root -p\n</code></pre> <ul> <li>Create database</li> </ul> <pre><code>CREATE DATABASE moodle DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n\nCREATE USER 'moodle-user'@'localhost' IDENTIFIED BY 'Cluster_p@ssw0rd';\n\nGRANT SELECT,INSERT,UPDATE,DELETE,CREATE,CREATE TEMPORARY TABLES,DROP,INDEX,ALTER ON moodle.* TO 'moodle-user'@'localhost';\n\nexit\n</code></pre>"},{"location":"setup/moodle/#config-apache2","title":"Config apache2","text":"<ul> <li>Edit file <code>sudo vi /etc/apache2/sites-available/000-default.conf</code></li> </ul> <pre><code>Listen 8888\n&lt;VirtualHost *:8888&gt;\n    # The ServerName directive sets the request scheme, hostname and port that\n    # the server uses to identify itself. This is used when creating\n    # redirection URLs. In the context of virtual hosts, the ServerName\n    # specifies what hostname must appear in the request's Host: header to\n    # match this virtual host. For the default virtual host (this file) this\n    # value is not decisive as it is used as a last resort host regardless.\n    # However, you must set it for any further virtual host explicitly.\n    #ServerName www.example.com\n\n    ServerAdmin webmaster@localhost\n    DocumentRoot /var/www/html/\n\n    # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,\n    # error, crit, alert, emerg.\n    # It is also possible to configure the loglevel for particular\n    # modules, e.g.\n    #LogLevel info ssl:warn\n\n    ErrorLog ${APACHE_LOG_DIR}/error.log\n    CustomLog ${APACHE_LOG_DIR}/access.log combined\n\n    # For most configuration files from conf-available/, which are\n    # enabled or disabled at a global level, it is possible to\n    # include a line for only one particular virtual host. For example the\n    # following line enables the CGI configuration for this host only\n    # after it has been globally disabled with \"a2disconf\".\n    #Include conf-available/serve-cgi-bin.conf\n&lt;/VirtualHost&gt;\n</code></pre> <ul> <li>Restart <code>apache2</code></li> </ul> <pre><code>sudo systemctl status apache2\n</code></pre> <ul> <li>Check IP and port config in moodle config</li> </ul> <pre><code>cd /var/www/html/moodle\nsudo vi config.php\n</code></pre> <ul> <li>Please ensure that <code>$CFG-&gt;wwwroot</code> has the <code>IP:port</code></li> </ul> <pre><code>&lt;?php  // Moodle configuration file\n\nunset($CFG);\nglobal $CFG;\n$CFG = new stdClass();\n\n$CFG-&gt;dbtype    = 'mariadb';\n$CFG-&gt;dblibrary = 'native';\n$CFG-&gt;dbhost    = 'localhost';\n$CFG-&gt;dbname    = 'moodle';\n$CFG-&gt;dbuser    = 'moodle-user';\n$CFG-&gt;dbpass    = 'Cluster_p@ssw0rd';\n$CFG-&gt;prefix    = 'mdl_';\n$CFG-&gt;dboptions = array (\n  'dbpersist' =&gt; 0,\n  'dbport' =&gt; '',\n  'dbsocket' =&gt; '',\n  'dbcollation' =&gt; 'utf8mb4_unicode_ci',\n);\n\n$CFG-&gt;wwwroot   = 'http://192.168.33.5/moodle';\n$CFG-&gt;dataroot  = '/var/moodledata';\n$CFG-&gt;admin     = 'admin';\n\n$CFG-&gt;directorypermissions = 0777;\n\nrequire_once(__DIR__ . '/lib/setup.php');\n\n// There is no php closing tag in this file,\n// it is intentional because it prevents trailing whitespace problems!\n</code></pre> <ul> <li>Change config in <code>sudo vi /etc/php/8.1/apache2/php.ini</code></li> </ul> <pre><code>max_input_vars=5000\n</code></pre> <ul> <li>Restart <code>apache2</code></li> </ul> <pre><code>sudo systemctl restart apache2\n</code></pre> <ul> <li>Open website and config: http://192.168.33.5:8888/moodle</li> <li>Follow this tutorial.</li> </ul>"},{"location":"setup/moodle/#install-oauth-plugin","title":"Install OAuth Plugin","text":"<ul> <li>Reference this tutorial. </li> <li>Install the Moodle OAuth plugin:</li> <li>Install by uploading the zip file at Site administration &gt; Plugins &gt; Install plugins</li> <li>Head over to Site administration &gt; Server &gt; OAuth provider settings &gt; Add new client</li> <li><code>client_id</code>: JupyterHub</li> <li> <p>callback URL to http://192.168.33.5/hub/oauth_callback</p> </li> <li> <p>Copy <code>client_token</code>:</p> </li> <li>Paste to config.yaml file</li> </ul>"},{"location":"setup/native-machine/","title":"Native machines","text":""},{"location":"setup/native-machine/#information","title":"Information","text":"<ul> <li>Ubuntu server/desktop 22.04 (server version is preferred)</li> <li>Name convension: <ul> <li>Headnode: cluster-hn</li> <li>Compute node: cluster-gn[1/2/3/..]<ul> <li>gn: GPU node</li> </ul> </li> </ul> </li> <li>NFS folder<ul> <li>NFS server is hosted on the Headnode</li> </ul> </li> <li>IP addresses:<ul> <li>Headnode:<ul> <li>br1: 192.168.33.5</li> <li>br2: 172.16.0.5</li> </ul> </li> <li>Compute node:<ul> <li>br0: 172.20.0.[10 + Node ID]</li> <li>br1: 192.168.33.[10 + Node ID]</li> <li>br2: 172.16.0.[10 + Node ID]</li> </ul> </li> </ul> </li> </ul>"},{"location":"setup/native-machine/#download-and-install-ubuntu","title":"Download and install Ubuntu","text":"<ul> <li>Download and install Ubuntu on the native machine</li> </ul>"},{"location":"setup/native-machine/#set-up-networking","title":"Set up networking","text":"<p>There are 3 networks: - br0: internal communication of virtual machines (setup later) - br1: high-speed network (bonding config) - br2: internet access (1/10GB network)</p>"},{"location":"setup/native-machine/#set-up-bonding","title":"Set up bonding","text":"<ul> <li>Check hardware</li> </ul> <pre><code>sudo lshw -C network\n</code></pre> <ul> <li>Install</li> </ul> <pre><code>ifconfig ens11 down\nifconfig ens12 down\n\nip link add bond0 type bond mode 802.3ad\n\nip link set ens11 master bond0\nip link set ens12 master bond0\n</code></pre>"},{"location":"setup/native-machine/#create-a-bridge-br1-100g-network-and-a-bridge-br2-110g-network","title":"Create a bridge br1 (100G network) and a bridge br2 (1/10G network)","text":"<ul> <li>Create a new file located at <code>/etc/netplan/01-netcfg.yaml</code></li> </ul> <pre><code>network:\n  ethernets:\n    ens11:\n      dhcp4: false\n      dhcp6: false\n    ens12:\n      dhcp4: false\n      dhcp6: false\n  bonds:\n    bond0:\n      dhcp4: false\n      interfaces:\n        - ens11\n        - ens12\n      parameters:\n        mode: balance-tlb\n        mii-monitor-interval: 100\n  version: 2\n  bridges:\n    br1:\n      dhcp4: false\n      dhcp6: false\n      interfaces: [ bond0 ]\n      addresses: [172.16.0.5/24]\n      nameservers:\n         addresses: []\n      routes:\n         - to: default\n           via: 172.16.0.1\n      mtu: 1500\n      parameters:\n        stp: true\n        forward-delay: 4\n    br2:\n      dhcp4: false\n      dhcp6: false\n      interfaces: [ eno1 ]\n      addresses: [192.168.33.5/24]\n      nameservers:\n         addresses: []\n      routes:\n         - to: default\n           via: 192.168.33.1\n      mtu: 1500\n      parameters:\n        stp: true\n        forward-delay: 4\n</code></pre> <ul> <li>Appyly change</li> </ul> <pre><code>sudo netplan generate\nsudo netplan apply\n</code></pre>"},{"location":"setup/native-machine/#check-bonding","title":"Check bonding","text":"<pre><code># after setting bonding, [bonding] is loaded automatically\nlsmod | grep bond\n\nip address show\nethtool bond0\n</code></pre>"},{"location":"setup/native-machine/#generate-an-ssh-key","title":"Generate an SSH key","text":"<pre><code>ssh-keygen -t rsa\n</code></pre>"},{"location":"setup/native-machine/#set-up-the-nfs","title":"Set up the NFS","text":""},{"location":"setup/native-machine/#server","title":"Server","text":"<ul> <li>Create a new mount point</li> </ul> <pre><code>sudo mkdir /mnt/nfs-hn\n</code></pre> <ul> <li> <p>Add line <code>/dev/sdb /mnt/nfs-hn ext4 defaults 0 0</code> in <code>/etc/fstab</code></p> </li> <li> <p>Install NFS server</p> </li> </ul> <pre><code>sudo apt install nfs-kernel-server\nsudo chown nobody:nogroup /mnt/nfs-ehn1\n</code></pre> <ul> <li>Edit exports <code>sudo nano /etc/exports</code></li> </ul> <pre><code>/mnt/nfs-hn/      172.16.0.0/24(rw,sync,no_subtree_check,no_root_squash,no_all_squash,insecure)\n</code></pre> <ul> <li>Restart NFS server</li> </ul> <pre><code>sudo systemctl restart nfs-kernel-server\n</code></pre>"},{"location":"setup/native-machine/#client","title":"Client","text":"<p>This is set up in all machines with a required access to the NFS folder.</p> <ul> <li>Install NFS client</li> </ul> <pre><code>sudo apt install nfs-common\n</code></pre> <ul> <li>Create folders</li> </ul> <pre><code>sudo mkdir -p /mnt/nfs\nsudo mkdir -p /mnt/local\n</code></pre> <ul> <li>Edit <code>fstab</code> file: <code>sudo vi /etc/fstab</code></li> </ul> <pre><code>/dev/sdb /mnt/local ext4 defaults 0 0\n172.16.0.5:/mnt/nfs-hn/   /mnt/nfs  nfs  defaults 1 2\n</code></pre> <ul> <li>Remount</li> </ul> <pre><code>sudo mount -av\n</code></pre>"},{"location":"setup/others/","title":"Others","text":"<p>This includes some setups which may need in the system.</p>"},{"location":"setup/others/#set-up-the-network-card-driver-mlnx_ofed","title":"Set up the network card driver (<code>mlnx_ofed</code>)","text":"<ul> <li>Download <code>mlnx_ofed</code></li> </ul> <pre><code>mkdir -p /home/cluster-gn1/network &amp; cd /home/cluster-gn1/network\nwget https://content.mellanox.com/ofed/MLNX_OFED-5.8-2.0.3.0/MLNX_OFED_LINUX-5.8-2.0.3.0-ubuntu22.04-x86_64.iso\n</code></pre> <ul> <li>Install the driver</li> </ul> <pre><code>sudo -i\n\nmount -o loop /home/cluster-gn1/network/MLNX_OFED_LINUX-5.8-2.0.3.0-ubuntu22.04-x86_64.iso /mnt\ncd /mnt\n./mlnxofedinstall --force\n/etc/init.d/openibd restart\n\numount /mnt\n</code></pre>"},{"location":"setup/vgpu/","title":"vGPU","text":""},{"location":"setup/vgpu/#install-dependencies","title":"Install dependencies","text":""},{"location":"setup/vgpu/#install-dependencies_1","title":"Install dependencies","text":"<pre><code>sudo apt install unzip\n</code></pre>"},{"location":"setup/vgpu/#download-driver","title":"Download driver","text":"<ul> <li>Download package <code>NVIDIA-GRID-Ubuntu-KVM-525.85.07-525.85.05-528.24.zip</code></li> <li>Create token for licensing</li> <li>Extract</li> </ul> <pre><code>mkdir -p /mnt/nfs/vGPU\nunzip NVIDIA-GRID-Ubuntu-KVM-525.85.07-525.85.05-528.24.zip -d /mnt/nfs/vGPU\n</code></pre>"},{"location":"setup/vgpu/#remove-current-nvidia-driver-if-existing","title":"Remove current NVIDIA driver if existing","text":"<pre><code>bash /mnt/local/NVIDIA-Linux-x86_64-450.216.04.run --uninstall\n</code></pre>"},{"location":"setup/vgpu/#host-machine-compute-node","title":"Host machine (compute node)","text":""},{"location":"setup/vgpu/#disable-nouveau-and-enable-nvfio","title":"Disable Nouveau and enable nvfio","text":"<pre><code>echo -e \"vfio\\nvfio_iommu_type1\\nvfio_pci\\nvfio_virqfd\" &gt;&gt; /etc/modules\necho \"blacklist nouveau\" &gt;&gt; /etc/modprobe.d/blacklist.conf\nupdate-initramfs -u -k all\n</code></pre>"},{"location":"setup/vgpu/#disable-x-server","title":"Disable X server","text":"<pre><code>sudo init 3\n</code></pre> <ul> <li>Ref: link</li> </ul>"},{"location":"setup/vgpu/#install-driver","title":"Install driver","text":"<pre><code>cd /mnt/nfs/vGPU/Host_Drivers\nsudo apt install ./nvidia-vgpu-ubuntu-525_525.85.07_amd64.deb\n</code></pre> <ul> <li>Ref: link</li> </ul>"},{"location":"setup/vgpu/#configuring-vgpu-deprecated","title":"Configuring vGPU [Deprecated]","text":"<ul> <li>Check domain/bus/slot/function</li> </ul> <pre><code>admin@cluter-gn1:~$ lsmod | grep vfio\nnvidia_vgpu_vfio       65536  272\nmdev                   28672  1 nvidia_vgpu_vfio\n\nadmin@cluter-gn1:~$ lspci | grep NVIDIA\n89:00.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] (rev a1)\n8a:00.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] (rev a1)\nb2:00.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] (rev a1)\nb3:00.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] (rev a1)\n\nadmin@cluter-gn1:~$ virsh nodedev-list --cap pci| grep 89_00_0\npci_0000_89_00_0\n\nadmin@cluter-gn1:~$ virsh nodedev-dumpxml pci_0000_89_00_0 | egrep 'domain|bus|slot|function'\n    &lt;domain&gt;0&lt;/domain&gt;\n    &lt;bus&gt;137&lt;/bus&gt;\n    &lt;slot&gt;0&lt;/slot&gt;\n    &lt;function&gt;0&lt;/function&gt;\n</code></pre>"},{"location":"setup/vgpu/#create-vgpu-manually","title":"Create vGPU manually","text":"<ul> <li>Change to the root user</li> </ul> <pre><code>sudo -i\n</code></pre> <ul> <li>Seach nvidia profiles<ul> <li><code>8Q</code> - 8G/vGPU</li> <li><code>16Q</code> - 16G/vGPU</li> </ul> </li> </ul> <pre><code>cd /sys/bus/pci/devices/0000:89:00.0/mdev_supported_types\ngrep -l \"V100DX-8Q\" nvidia-*/name\n</code></pre> <ul> <li>The output:</li> </ul> <pre><code>nvidia-197/name\n</code></pre> <ul> <li>Check available instances, the result should be greater than 0</li> </ul> <pre><code>cat nvidia-197/available_instances\n</code></pre> <ul> <li>Generate uuid for the vGPU</li> </ul> <pre><code># uuidgen\nb87b1cd3-feb8-4ca6-88af-33b3c9f81425\n</code></pre> <ul> <li>Write the UUID that you obtained in the previous step to the <code>create</code> file in the registration information directory for the vGPU type that you want to create </li> </ul> <pre><code>echo \"b87b1cd3-feb8-4ca6-88af-33b3c9f81425\" &gt; nvidia-197/create\n</code></pre> <ul> <li>Make the mdev device file that you created to represent the vGPU persistent.</li> </ul> <pre><code>mdevctl define --auto --uuid b87b1cd3-feb8-4ca6-88af-33b3c9f81425\n</code></pre> <ul> <li>Confirm that the vGPU was created</li> </ul> <pre><code>ls -l /sys/bus/mdev/devices/\n</code></pre> <p>or</p> <pre><code>mdevctl list\n</code></pre>"},{"location":"setup/vgpu/#create-all-vgpu","title":"Create all vGPU","text":"<pre><code>cd /mnt/nfs/vGPU\nsudo ./admin-create-all-vgpus.sh -r 8\n</code></pre> <ul> <li><code>-r 8</code>: choose 8GB of vGPU memory</li> </ul>"},{"location":"setup/vgpu/#adding-one-or-more-vgpus-to-a-linux-with-kvm-hypervisor-vm-by-using-virsh","title":"Adding One or More vGPUs to a Linux with KVM Hypervisor VM by Using virsh","text":"<pre><code>virsh edit vgn1\n</code></pre> <ul> <li>Add device entries</li> </ul> <pre><code>&lt;device&gt;\n...\n    &lt;hostdev mode='subsystem' type='mdev' model='vfio-pci'&gt;\n      &lt;source&gt;\n        &lt;address uuid=''/&gt;\n      &lt;/source&gt;\n    &lt;/hostdev&gt;\n    &lt;hostdev mode='subsystem' type='mdev' model='vfio-pci'&gt;\n      &lt;source&gt;\n        &lt;address uuid=''/&gt;\n      &lt;/source&gt;\n    &lt;/hostdev&gt;\n&lt;/device&gt;\n</code></pre> <ul> <li>Start/Restart the VM</li> </ul> <pre><code>virsh start vgn1\n</code></pre>"},{"location":"setup/vgpu/#vms","title":"VMs","text":""},{"location":"setup/vgpu/#access-to-the-vm-via-virsh","title":"Access to the VM via <code>virsh</code>","text":"<pre><code>admin@cluter-gn1:/mnt/nfs/vGPU$ virsh list --all\n Id   Name   State\n----------------------\n 2    vgn1   running\n\nadmin@cluter-gn1:/mnt/nfs/vGPU$ virsh console vgn1\nConnected to domain 'vgn1'\nEscape character is ^] (Ctrl + ])\n\nadmin@cluter-vgn1:~$ ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: enp1s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 52:54:00:05:41:79 brd ff:ff:ff:ff:ff:ff\n    inet 172.20.0.101/24 brd 172.20.0.255 scope global enp1s0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5054:ff:fe05:4179/64 scope link \n       valid_lft forever preferred_lft forever\n3: enp2s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 52:54:00:89:d9:16 brd ff:ff:ff:ff:ff:ff\n    inet 172.16.0.41/24 brd 172.16.0.255 scope global enp2s0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5054:ff:fe89:d916/64 scope link \n       valid_lft forever preferred_lft forever\n4: enp20s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 52:54:00:51:d3:51 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.33.101/24 brd 192.168.33.255 scope global enp20s0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5054:ff:fe51:d351/64 scope link \n       valid_lft forever preferred_lft forever\n</code></pre> <ul> <li>Install SSH server</li> </ul> <pre><code>sudo apt install openssh-server\n</code></pre> <ul> <li>Exit console: <code>Ctrl + ]</code></li> </ul>"},{"location":"setup/vgpu/#access-to-the-vm-via-ssh","title":"Access to the VM via <code>ssh</code>","text":"<ul> <li>Copy ssh key</li> </ul> <pre><code>ssh-copy-id -i ~/.ssh/id_rsa.pub ubuntu@172.16.0.101\n</code></pre> <ul> <li>SSH to the VM</li> </ul> <pre><code>ssh ubuntu@172.16.0.101\n</code></pre> <ul> <li>Copy NVIDIA Guest drive and token</li> </ul> <pre><code>scp /mnt/nfs/vGPU/Guest_Drivers/nvidia-linux-grid-525_525.85.05_amd64.deb ubuntu@172.16.0.101:/home/admin\nscp /mnt/nfs/vGPU/client_configuration_token_03-23-2023-09-07-06.tok ubuntu@172.16.0.101:/home/admin\n</code></pre> <ul> <li>Install NVIDIA driver</li> </ul> <pre><code>sudo apt install ./nvidia-linux-grid-525_525.85.05_amd64.deb\n</code></pre> <ul> <li>Change FeatureType from 0 to 2</li> </ul> <pre><code>sudo nano /etc/nvidia/gridd.conf\n</code></pre> <pre><code># Description: Set Feature to be enabled\n# Data type: integer\n# Possible values:\n#    0 =&gt; for unlicensed state\n#    1 =&gt; for NVIDIA vGPU (Optional, autodetected as per vGPU type)\n#    2 =&gt; for NVIDIA RTX Virtual Workstation\n#    4 =&gt; for NVIDIA Virtual Compute Server\n# All other values reserved\nFeatureType=2\n</code></pre> <ul> <li>Restart VM</li> </ul> <pre><code>sudo reboot\n</code></pre> <ul> <li>SSH to VM</li> <li>Copy token</li> </ul> <pre><code>sudo cp client_configuration_token_03-23-2023-09-07-06.tok /etc/nvidia/ClientConfigToken/\n</code></pre> <ul> <li>Change mode</li> </ul> <pre><code>chmod 744 /etc/nvidia/ClientConfigToken/client_configuration_token_03-23-2023-09-07-06.tok\n</code></pre> <ul> <li>Restart nvidia-gridd deamon</li> </ul> <pre><code>sudo systemctl restart nvidia-gridd.service\n</code></pre> <ul> <li>Test</li> </ul> <pre><code>nvidia-smi -q\n</code></pre> <ul> <li>Rebooting the VM may save your time when the vGPU does not recognize the license</li> </ul> <pre><code>sudo reboot\n</code></pre>"}]}